---
title: "Simple Linear Regression---Description"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \definecolor{umn}{HTML}{FF2D21}
   - \definecolor{myorange}{HTML}{EA6153}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \usepackage{caption}
   - \captionsetup[table]{textfont={it}, labelfont={}, singlelinecheck=false, labelsep=newline}
   - \captionsetup[figure]{textfont={}, labelfont={it}, singlelinecheck=false, labelsep=period}  
output: 
  pdf_document:
    latex_engine: xelatex
    highlight: zenburn
    fig_width: 6
    fig_height: 6
mainfont: "Sabon"
sansfont: "Futura"
monofont: Inconsolata    
urlcolor: "umn"
bibliography: epsy8251.bib
csl: apa-single-spaced.csl
---

\frenchspacing

<!-- LaTeX definitions -->

\mdfdefinestyle{mystyle}{userdefinedwidth=5in, align=center, backgroundcolor=yellow, roundcorner=10pt, skipabove=2em}

\mdfdefinestyle{mystyle2}{userdefinedwidth=5.5in, align=center, skipabove=10pt, topline=false, bottomline=false, 
linecolor=myorange, linewidth=5pt}

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE)
opts_knit$set(width=85)
options(scipen=5)
```


# Introduction and Research Question

In this set of notes, you will begin your foray into regression analysis. To do so, we will use the *riverview.csv* data to examine whether education level is related to income. The data contain five attributes collected from a random sample of $n=32$ employees working for the city of Riverview, a hypothetical midwestern city (see the [data codebook](http://zief0002.github.io/epsy-8251/codebooks/riverview.html)). To begin, we will load several libraries and import the data into an object called `city`.

```{r preparation, warning=FALSE, message=FALSE}
# Load libraries
library(corrr)
library(dplyr)
library(ggplot2)
library(readr)
library(sm)

# Read in data
city = read_csv(file = "~/Documents/github/epsy-8251/data/riverview.csv")
head(city)
```

# Data Exploration

Any analysis should start with an initial exploration of the data. During this exploration, you should examine each of the variables that you will be including in the regression analysis. This will help you understand results you get in later analyses, and will also help foreshadow potential problems with the analysis. [This blog post](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/) describes initial ideas of data exploration reasonably well. You could also refer to almost any introductory statistics text for additional detail. 

It is typical to begin by exploring the distribution of each variable used in the analysis separately. These distributions are referred to as *marginal distributions*. After that, it is appropriate to explore the relationships between the variables. 

\newpage

## Income

To begin this exploration, we will examine the marginal distribution of employee incomes. We can plot a marginal distribution using the `sm.density()` function from the **sm** package.

```{r fig.width=6, fig.height=6, out.width='50%', fig.cap="Density plot of employee incomes.", fig.pos='H', fig.align='center'}
sm.density(city$income, xlab = "Income (in thousands of dollars)")
```

This plot suggests that the distribution of employee incomes is unimodal and most of the incomes are between roughly \$50,000 and \$70,000. The rug at the bottom of the plot (the small vertical line segments) show the actual sample incomes. The smallest income in the sample is about \$25,000 and the largest income is over \$80,000. (We could find the exact values using the `summary()` function.) This suggests there is a fair amount of variation in the data. 

To further summarize the distribution, it is typical to compute and report summary statistics such as the mean and standard deviation. One way to compute these values is to use functions from the **dplyr** library.

```{r}
city %>% 
  summarize(
    M = mean(income), 
    SD = sd(income)
    )
```

\newpage

Describing this variable we might write,

\begin{mdframed}[style=mystyle2]
The marginal distribution of income is unimodal with a mean of \$53,742. There is variation in employees' salaries (SD = \$14,553). 
\end{mdframed}


## Education Level

We will also examine the distribution of the education level variable.

```{r fig.width=6, fig.height=6, out.width='50%', fig.cap="Density plot of employee education levels.", fig.pos='H', fig.align='center'}
# Plot
sm.density(city$education, xlab = "Education Level")
```


```{r}
# Summary statistics
city %>% 
  summarize(
    M = mean(education), 
    SD = sd(education)
    )
```

\newpage

Again, we might write,

\begin{mdframed}[style=mystyle2]
The marginal distribution of education is unimodal with a mean of 16 years. There is variation in employees' level of education (SD = 4.4).
\end{mdframed}


## Relationship Between Variables

Although examining the marginal distributions is an important first step, those descriptions do not help us directly answer our research question. To better understand any relationship between income and education level we need to explore how the distribution of income differs as a function of education. To do this, we will create a scatterplot of incomes versus education. 


### Scatterplot

```{r fig.width=6, fig.height=6, out.width='3.5in', fig.cap="Scatterplot displaying the relationship between employee education levels and incomes.", fig.pos='H', fig.align='center'}
ggplot( data = city, aes(x = education, y = income) ) +
  geom_point() +
  theme_bw() +
  xlab("Education (in years)") +
  ylab("Income (in U.S. dollars)")
```


\newpage

The plot suggests a relationship (at least for these employees) between level of education and income. When describing the relationship we want to touch on four characteristics of the relationship:

- Functional form of the relationship
- Direction
- Strength
- Observations that do not fit the trend (outliers)


### Correlation

To numerically summarize relationships between variables, we typically compute correlation coefficients. The correlation coefficient is a quantification of the direction and strength of the relationship. (It is important to note that the correlation coefficient is only an appropriate summarization of the relationship if the functional form of the relationship is linear.) 

To compute the correlation coefficient, we use the `correlate()` function from the **corrr** package. We can use the dplyr-type syntax to select the variables we want correlations between, and then pipe that into the `correlate()` function. Typically the response (or outcome) variable is the first variable provided in the `select()` function, followed by the predictor.

```{r}
city %>%
  select(income, education) %>%
  correlate()
```

When reporting the correlation coefficient is is conventional to use a lower-case $r$ and report the value to two decimal places. Subscripts are also generally used to indicate the variables. For example,

$$
r_{\mathrm{education,~income}} = 0.79
$$

Combining the information culled from the scatterplot with that of the correlation analysis, we could summarize the relationship between education level and income as,

\begin{mdframed}[style=mystyle2]
There is a strong, positive, linear relationship between education level and income ($r = .79$). This suggests that city employees with lower education levels tend to have lower incomes, on average, than employees with higher education levels.
\end{mdframed}


# Statistical Model

Since the relationship's functional form seems reasonably linear, we will use a *linear model* to describe the data. We can express this model mathematically as,

$$
Y_i = \beta_0 + \beta_1(X_i) + \epsilon_i.
$$

\newpage

In this equation,

- $Y_i$ is the outcome/response value; it has an $i$ subscript because it can vary across cases/individuals.
- $\beta_0$ is the intercept of the line that best fits the data; it does not vary across individuals.
- $\beta_1$ is the slope of the line that best fits the data; it does not vary across individuals.
- $X_i$ is the predictor value; it has an $i$ subscript because it can vary across cases/individuals.
- $\epsilon_i$ is the error term; it has an $i$ subscript because it can vary across cases/individuals.


## Regression Equation

The regression model can be separated into two components: a *systematic* (or fixed) component and a *random* (or stochastic) component. 

$$
Y_i = \underbrace{\beta_0 + \beta_1(X_i)}_{\substack{\text{Systematic} \\ \text{(Fixed)}}} + \underbrace{\epsilon_i}_{\substack{\text{Random} \\ \text{(Stochastic)}}} 
$$

The systematic (fixed) part of the equation gives the expected value (or mean value) of $Y$ given a particular $X$-value. The notation for the expected value $Y$ is $E(Y_i)$. We express this mathematically as, 

$$
E(Y_i \vert X) = \beta_0 + \beta_1(X_i).
$$

Sometimes statisticians will use the notation of $\mu_{Y \vert X_i}$ rather than the expected value notation. This is read, *the mean value of Y given a particular X-value*. (Another name for this is the conditional mean of $Y$.) Thus, the same equation can be written as,

$$
\mu_{Y \vert X_i} = \beta_0 + \beta_1(X_i).
$$


The terms $\beta_0$ and $\beta_1$ are referred to as the regression parameters. One of the primary goals of a regression analysis is to estimate the values of the regression parameters (i.e., the intercept and slope terms). (Note that the equation for the expected value does not include any error terms.) 

## Residuals

Now we can re-write the statistical model, substituting $\mu_{Y \vert X_i}$ in to the systematic part of the model.

$$
\begin{split}
Y_i &= \beta_0 + \beta_1(X_i) + \epsilon_i \\
Y_i &= \mu_{Y \vert X_i} + \epsilon_i 
\end{split}
$$

This equation implies that each observed $Y$-value is the sum of the conditional mean value of $Y$ (which is based on the $X$-value) and some residual (or error) term. Re-arranging the terms, we can mathematically express the residual term as,

$$
\epsilon_i = Y_i - \mu_{Y \vert X_i}
$$

To compute an observation's residual, we compute the difference between the observation's $Y$-value and its conditional mean value. When the observed value of $Y$ is larger than the conditional mean value of $Y$ the residual term will be positive (underprediction). If the observed value of $Y$ is smaller than the conditional mean value of $Y$ the residual term will be negative (overprediction).

### Why is there an error term in the statistical model?

We use a single line to describe the relationship between education and income. This line is the same for all of the observations in the sample. For example, look at the figure below which shows the relationship between education and income, but this time also includes the regression line.

```{r echo=FALSE, fig.width=6, fig.height=6, out.width='3.5in', fig.cap="Scatterplot displaying the relationship between employee education levels and incomes. The OLS fitted regression line is also displayed.", fig.pos='H', fig.align='center'}
ggplot( data = city, aes(x = education, y = income) ) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(x = 10, y = 37831, color = "blue", size = 4) +
  theme_bw() +
  xlab("Education (in years)") +
  ylab("Income (in U.S. dollars)")
```

Consider all the employees that have an education level of 10 years. For all three of them we would predict an income of approximately \$37,800. This is denoted by the blue point on the line. The error term allows for discrepancy between the predicted $Y$ and the observed $Y$, which allows us to recover our observed value of the response variable from the model.

Graphically, the residual is represented by the vertical distance between the line and a given point on the scatterplot. Some of those points are above the line (they have a positive residual) and some are below the line (they have a negative residual). Also note that for some observations the error term is smaller than for others.

## Notation

The regression model and the fitted regression equation describe the linear relationship in the *population*. Greek letters indicate a *parameter* (a summary of the population). That is why we use the Greek letter $\beta$ when we notate the regression model. It is important to note that outside of the $X$- and $Y$-values, the parameters ($\beta_0$ and $\beta_1$) and hence the residual values ($\epsilon_i$) are *unobserved*. 

In most statistical analyses, you will use a *sample* of data (not the entire population). When we summarize a sample, it is referred to as a *statistic*, and we use either Roman letters or a Greek letter with a hat. Moreover, we also tend to express the conditional mean using the notation $\hat{Y}_i$. This notation indicates that the summary measure is an estimate of the parameter that resulted from fitting the model to the data, also referred to as the *fitted equation*. 

\newpage

We can represent the fitted equation mathematically as:

$$
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1(X_i)
$$

The regression parameter estimates are also referred to as *regression coefficients*. Some people use Roman letters when referring to sample estimates.

$$
\hat{Y}_i = b_0 + b_1(X_i)
$$

Sometimes $\hat{Y}$ is referred to as the predicted value of $Y$. We can also use the estimated regression coefficients to get estimates for the observed residuals,

$$
e_i = Y_i - \hat{Y}_i
$$

Each observation's observed residual is the difference between that observation's observed outcome and its predicted value (given it's $X$-value). 

# Regression Estimates Using R

To fit the regression model to data using R, we will use the `lm()` function. The syntax for this function looks like this:

> `lm(`**outcome** ~ `1 + ` **predictor**, `data =` **dataframe**`)`

where **outcome** is the name of the outcome/response variable, **predictor** is the name of the predictor variable, and **dataframe** is the name of the data frame. (The one on the right side of the tilde tells R to include the intercept in its computation.) When we fit a regression model in R, we will also assign the output to a new object in R. Below, we fit the model using education level to predict income. Here the output is assigned to an object called `lm.1`. We can print the regression parameter estimates by typing the `lm()` object name and hitting enter.

```{r}
# Fit regression model
lm.1 = lm(income ~ 1 + education, data = city)

# Print regression coefficients
lm.1
```

Here the parameter estimates (or regression coefficients) are:

- $\hat{\beta}_0 = 11.321$
- $\hat{\beta}_1 = 2.651$

Remember that these are estimates and need the hats. 

\newpage

The fitted regression equation is:

$$
\hat{\mathrm{Income}} = 11.321 + 2.651(\mathrm{Education~Level})
$$

## Intercept Interpretation

The estimate for the intercept was 11.321. Graphically, this value indicates the $y$-value where the line passes through the $y$-axis (i.e., $y$-intercept). As such, it gives the predicted value of $Y$ when $X = 0$. Algebraically we get the same thing if we substitute 0 in for $X_i$ in the estimated regression equation.

$$
\begin{split}
\hat{Y}_i &= \hat{\beta}_0 + \hat{\beta}_1(0) \\
\hat{Y}_i &= \hat{\beta}_0 
\end{split}
$$

To interpret this value, we use that same idea. Namely, the predicted income for all employees that have an education level of 0 years is 11.321 thousand dollars. Or, directly,

\begin{mdframed}[style=mystyle2]
The predicted income for all employees that have an education level of 0 years is \$11,321.
\end{mdframed}


## Slope Interpretation

Recall from algebra that the slope of a line describes the change in $Y$ versus the change in $X$. In regression, the slope describes the *predicted* change in $\hat{Y}$ for a one-unit difference in $X$. 

$$
\hat{\beta}_1 = \frac{\Delta\hat{Y}}{\Delta X} = \frac{2.651}{1}
$$

In our example, each one-year difference in education level is associated with a 2.651 thousand dollar  difference in predicted income. Or directly, 

\begin{mdframed}[style=mystyle2]
Each one-year difference in education level is associated with a \$2,651 difference in predicted income.
\end{mdframed}

To better understand this, consider three city employees. The first employee has an education level of 10 years. The second has an education level of 11 years, and the third has an education level of 12 years. Now let's compute each employee's predicted income.

$$
\begin{split}
\mathbf{Employee~1:~}\hat{\mathrm{Income}} &= 11.321 + 2.651(10) \\
&= 37.831
\end{split}
$$



$$
\begin{split}
\mathbf{Employee~2:~}\hat{\mathrm{Income}} &= 11.321 + 2.651(11) \\
&= 40.482
\end{split}
$$


$$
\begin{split}
\mathbf{Employee~3:~}\hat{\mathrm{Income}} &= 11.321 + 2.651(10) \\
&= 43.133
\end{split}
$$

Each of the employee's education levels differ by one year (10 to 11 to 12). The difference in predicted incomes for these employees differs by 2.651 thousand dollars. 

# Using the Regression Equation

Consider the twelfth case in the data frame.

```{r}
city %>%
  filter(row_number() == 12)
```

This employee has an education level of fourteen years ($X_{12}=14$). His income is 64.926 thousand dollars ($Y_{12}=64.926$). Using the fitted equation, we can compute that employee's predicted income as,

```{r}
# Y_hat = b0 + b1 * X
11.321 + 2.651 * 14
```

$\hat{Y}_{12} = 48.435$. 

Given his education level of 14 years, this employee has a predicted income of \$48,435. We can also compute that employee's residual.

```{r}
# e = Y - Y_hat
64.926 - 48.435
```


The positive residual, $e_{12} = 16.491$, suggests that this employee earns \$16,491 more than would be expected for a city employee with 14 years of formal education. We can also represent these values graphically.

```{r echo=FALSE, fig.width=6, fig.height=6, out.width='3in', fig.cap="Plot displaying the OLS fitted regression line (blue) between employee education levels and incomes. The 12th employee's observed data (black dot) is plotted, and a visual representation of the employee's residual (red line) is also displayed.", fig.pos='H', fig.align='center'}
ggplot(data = city, aes(x = education, y = income)) +
  geom_segment(x = 14, xend = 14, y = 48.435, yend = 64.926, color = "darkred") +
  geom_point(x = 14, y = 64.926, size = 4) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(x = 14, y = 48.435, color = "blue", size = 4) +
  annotate("text", x = 15, y = 48.435, label = "hat(Y)[12]", parse = TRUE) +
  annotate("text", x = 15, y = 64.926, label = "Y[12]", parse = TRUE) +
  annotate("text", x = 15, y = 56.6805, label = "hat(epsilon)[12]", parse = TRUE) +
  theme_bw() +
  xlab("Education (in years)") +
  ylab("Income (in U.S. dollars)")
```

\newpage

# Conditional Averages

Recall that another way to think about the predicted value ($\hat{Y}$) is it describes the mean value of $Y$ for *all* cases with a particular $X$ value. For example, using the values from the previous example we found that $\hat{Y}_{12} = 48.435$ when $X_{12}=14$. Using the idea of conditional means, we could interpret this as,

\begin{mdframed}[style=mystyle2]
The estimated average income for all city employees having 14 years of education is \$48,435.
\end{mdframed}

To help better understand the idea of conditional means, consider the following plot:

```{r echo=FALSE, out.width='3.5in', out.height='3.5in', fig.align='center', fig.pos='H', fig.cap="Plot displaying conditional distribution of $Y$ at several $X$ values. The OLS fitted regression line (dotted) is also shown. The red points show the mean value of $Y$ for each conditional distribution."}
include_graphics("images/conditional-means.png")
```

At each value of $X$ there is a distribution of $Y$. For example, there would be a distribution of incomes for the employees with an education level of 10 years (in the population). There would be another distribution of incomes for the employees with an education level of 11 years (in the population). And so on. 

The regression equation describes the pattern of conditional means. As such, we write the fitted equation using means rather than $\hat{Y}$,

$$
\mu_{Y|X} = \beta_0 + \beta_1(X_i)
$$

The first part is read as, "the mean of $Y$ given $X$", or "the mean of $Y$ conditioned on $X$". Sometimes the mean of a population is denoted as $E(Y)$, or the expected value of $Y$. Then you might see the regression equation written as,

$$
E(Y|X) = \beta_0 + \beta_1(X_i)
$$

\newpage

When we assume a linear functional form for the model, we are saying that *the mean value of $Y$ differs by a constant amount for each one-unit difference in $X$*. In other words, the difference between the mean income for those employees who have ten years of education and those that have 11 years of education *is the same as* the difference between the mean income for those employees who have 17 years of education and those that have 18 years of education.

## Intercept (Re-visited)

Using the idea of conditional means, we can re-visit the interpretation of the intercept, which we had said was the predicted $Y$ for a person with an $X$-value of zero. Now we can say that the intercept is the predicted mean income for all employees with zero years of formal education.

```{r echo=FALSE, out.width='3.5in', out.height='3.5in', fig.align='center', fig.pos='H', fig.cap="Plot displaying conditional distribution of $Y$ at $X=0$. The OLS fitted regression line (dotted) is also shown. The red points show the mean value of $Y$ for this conditional distribution---which corresponfds to the intercept value of the regression line."}
include_graphics("images/conditional-means-intercept.png")
```

## Slope (Re-visited)

Using the idea of conditional means, we can also re-visit the interpretation of the slope, which we had said was the predicted difference in $Y$ for employees with a one-year difference in education. Now we can say that the slope is the predicted difference in mean incomes between employees with education levels that differ by one year.

```{r echo=FALSE, out.width='3.5in', out.height='3.5in', fig.align='center', fig.pos='H', fig.cap="Plot displaying conditional distribution of $Y$ at $X=0$ and $X=1$. The OLS fitted regression line (dotted) is also shown. The red points show the mean value of $Y$ for these conditional distributions---the relative change which corresponds to the slope value of the regression line."}
include_graphics("images/conditional-means-slope.png")
```

\begin{mdframed}[style=mystyle]
In general, when interpreting the slope and intercept, you should use the conditional mean interpretations.
\end{mdframed}


